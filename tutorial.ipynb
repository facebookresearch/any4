{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cc49735",
   "metadata": {},
   "source": [
    "# ðŸ§  any4 Quantization Tutorial\n",
    "\n",
    "This tutorial demonstrates:\n",
    "- Running inference on a Hugging Face model\n",
    "- Applying `any4` quantization from Meta\n",
    "- Benchmarking speed and memory\n",
    "- Evaluating performance with `lm-eval` and BigCode Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e10b84f",
   "metadata": {},
   "source": [
    "## ðŸ“¦ 1. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e8047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "device = \"cuda\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "# Avoid HF warnings when pad token is missing\n",
    "model.generation_config.pad_token_id = model.generation_config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a022e239",
   "metadata": {},
   "source": [
    "## ðŸ”¶ 2. Inference with BF16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2033cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, do_sample=True, max_new_tokens=256)\n",
    "text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90630a16",
   "metadata": {},
   "source": [
    "## ðŸ“Š 3. Benchmarking (BF16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53158cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_model_size\n",
    "\n",
    "model_size = get_model_size(model)\n",
    "print(f\"Model Size: {model_size / 2**30:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import benchmark_cuda_only_in_ms\n",
    "\n",
    "model_cuda_time = benchmark_cuda_only_in_ms(model, warmup=0, iters=1, **inputs, do_sample=True, max_new_tokens=256)\n",
    "print(f\"GPU Time: {model_cuda_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b9e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import benchmark_in_ms\n",
    "\n",
    "model_total_time = benchmark_in_ms(model, warmup=0, iters=1, **inputs, do_sample=True, max_new_tokens=256)\n",
    "print(f\"Total Time: {model_total_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5d6de4",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ 4. Evaluation (BF16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27acb906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on LM Harness: PIQA and ARC-Easy\n",
    "import json\n",
    "from lm_eval import simple_evaluate\n",
    "\n",
    "results = simple_evaluate(\n",
    "    model=\"hf\",\n",
    "    model_args={\n",
    "        \"pretrained\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"batch_size\": 8\n",
    "    },\n",
    "    tasks=[\"piqa\", \"arc_easy\"],\n",
    ")\n",
    "print(json.dumps(results[\"results\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b265ffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on BigCode Humaneval\n",
    "import argparse\n",
    "from datetime import timedelta\n",
    "from accelerate import Accelerator, InitProcessGroupKwargs\n",
    "\n",
    "import bigcode_eval\n",
    "import bigcode_eval.evaluator\n",
    "from bigcode_eval.arguments import EvalArguments\n",
    "from eval import bigcode_default_args\n",
    "\n",
    "accelerator = Accelerator(InitProcessGroupKwargs(timeout=timedelta(weeks=52)))\n",
    "bigcode_evaluator = bigcode_eval.evaluator.Evaluator(\n",
    "    accelerator=accelerator,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=argparse.Namespace(**bigcode_default_args),\n",
    ")\n",
    "\n",
    "result = bigcode_evaluator.evaluate(\"humaneval\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc46e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Open Pile (Perplexity)\n",
    "from data import eval_perplexity, task_dataset_configs\n",
    "\n",
    "result = eval_perplexity(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=1,\n",
    "    max_seq_len=2048,\n",
    "    num_batches=10,\n",
    "    **task_dataset_configs[\"pile-clean\"]\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f83dab0",
   "metadata": {},
   "source": [
    "## ðŸ§® 5. Apply any4 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c337724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantize import any4\n",
    "\n",
    "# Apply any4 quantization to the model\n",
    "model = any4(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb4679",
   "metadata": {},
   "source": [
    "Now, `Linear` layers inside the model are replaced with `Any4Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f132bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17ffcbd",
   "metadata": {},
   "source": [
    "## ðŸ”· 6. Inference with Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c802509",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, do_sample=True, max_new_tokens=256)\n",
    "text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e64195",
   "metadata": {},
   "source": [
    "## ðŸ“Š 7. Benchmarking (Quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5696182",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = get_model_size(model)\n",
    "print(f\"Model Size: {model_size / 2**30:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eb95ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cuda_time = benchmark_cuda_only_in_ms(model, warmup=0, iters=1, **inputs, do_sample=True, max_new_tokens=256)\n",
    "print(f\"GPU Time: {model_cuda_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_total_time = benchmark_in_ms(model, warmup=0, iters=1, **inputs, do_sample=True, max_new_tokens=256)\n",
    "print(f\"Total Time: {model_total_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623058da",
   "metadata": {},
   "source": [
    "> âœ… **Model size reduced** from ~2.79 GB â†’ ~1.47 GB  \n",
    "> âœ… **GPU time reduced** from ~20.52 ms â†’ ~18.02 ms  \n",
    "> âœ… **Total latency reduced** from ~56.94 ms â†’ ~37.05 ms  \n",
    "\n",
    "*Note: The embedding and LM head are not quantized, which limits size reduction on small models like Llama 3.2B.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d3b254",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ 8. Evaluation (Quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17495d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on LM Harness: PIQA and ARC-Easy\n",
    "results = simple_evaluate(\n",
    "    model=\"hf\",\n",
    "    model_args={\n",
    "        \"pretrained\": model,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"batch_size\": 8\n",
    "    },\n",
    "    tasks=[\"piqa\", \"arc_easy\"],\n",
    ")\n",
    "print(json.dumps(results[\"results\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afbad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on BigCode Humaneval\n",
    "bigcode_evaluator = bigcode_eval.evaluator.Evaluator(\n",
    "    accelerator=accelerator,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=argparse.Namespace(**bigcode_default_args),\n",
    ")\n",
    "result = bigcode_evaluator.evaluate(\"humaneval\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a99b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Open Pile (Perplexity)\n",
    "result = eval_perplexity(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=1,\n",
    "    max_seq_len=2048,\n",
    "    num_batches=10,\n",
    "    **task_dataset_configs[\"pile-clean\"]\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499deb2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## âœ… 9. Conclusion\n",
    "\n",
    "`any4` delivers:\n",
    "- **Model size reduction**\n",
    "- **Faster inference**\n",
    "- **Minimal accuracy loss**\n",
    "\n",
    "This makes it a practical choice for deploying LLMs efficiently on GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1084e680",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
